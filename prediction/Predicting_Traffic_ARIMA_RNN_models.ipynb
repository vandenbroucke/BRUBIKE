{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import xgboost as xgb\n",
    "import xgboost\n",
    "import tensorflow\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras import regularizers\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import LSTM\n",
    "from tensorflow.python.keras import initializers\n",
    "from datetime import datetime as dt\n",
    "from utils import *\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from features_engineering import TIME_SERIES_FEATURES_ENGINEERING\n",
    "import sklearn\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from matplotlib import pyplot\n",
    "from statsmodels.tsa.arima_model import ARIMA    \n",
    "import numpy as np\n",
    "import math\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "start=dt.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process weathertypes, remove empty windspeed and store (ignore if it's already done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you want to generate the one_hot file uncomment the last line of this cell\n",
    "def update_one_hot_data(combined_path,weather_combined_path):\n",
    "    if not check_if_the_tsv_hot_file_already_has_processed_columns(weather_combined_path):        \n",
    "        \n",
    "        df = pd.read_csv(combined_path,\n",
    "                         sep='\\t',\n",
    "                         header=0) \n",
    "        weather_unique_combinations = df.weather_condition.unique()\n",
    "\n",
    "        #Get list of all unique weather types\n",
    "        types = []\n",
    "        for el in weather_unique_combinations:\n",
    "            for wc in el.split('.'):\n",
    "                if(wc != ''):            \n",
    "                    types.append(wc.strip())\n",
    "        true_unique =  set(types)\n",
    "\n",
    "        #Add columns with default value 0 for all unique weather types\n",
    "        for unique_weather_type in true_unique:\n",
    "            df[unique_weather_type]=0\n",
    "\n",
    "        #Loop over all records and set value to 1 for their corresponding weather_types\n",
    "        for index, row in df.iterrows():\n",
    "            row_types = []\n",
    "            for wc in row[\"weather_condition\"].split('.'):\n",
    "                if(wc != ''):            \n",
    "                    row_types.append(wc.strip())\n",
    "            for t in row_types:\n",
    "                df.at[index,t]=1\n",
    "\n",
    "\n",
    "        #remove empty windspeeds\n",
    "        df = df[df.wind_speed.apply(lambda x: str(x).isnumeric())]\n",
    "\n",
    "        #remove original weather_condition column and store, to avoid rerun\n",
    "        df =  df.drop(columns=\"weather_condition\")\n",
    "        df.to_csv(weather_combined_path,\n",
    "                  sep='\\t',\n",
    "                  index=False,\n",
    "                  header=True) \n",
    "        TIME_SERIES_FEATURES_ENGINEERING(weather_combined_path)\n",
    "    else:\n",
    "        print(\"File already processed.\")\n",
    "        \n",
    "combined_path = \".././data/combined_data.tsv\"\n",
    "weather_combined_path=\".././data/combined_one_hot_data.tsv\"\n",
    "#update_one_hot_data(combined_path,weather_combined_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loadin data with one_hot weather types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101461, 48)\n",
      "Selecting data for bikes station on latitude 50.85363\n",
      "(15277, 48)\n"
     ]
    }
   ],
   "source": [
    "def select_data_from_lat_csv(data,latitude):\n",
    "\n",
    "    return data[data['latitude']==latitude]\n",
    "def obtain_latitudes_list(latitude_data):\n",
    "    return latitude_data.unique()\n",
    "weather_combined_path=\".././data/combined_one_hot_data.tsv\"\n",
    "df = pd.read_csv(weather_combined_path,\n",
    "                 sep='\\t',\n",
    "                 header=0)\n",
    "print(df.shape)\n",
    "lat_list=obtain_latitudes_list(df.latitude)\n",
    "pole=5\n",
    "df=select_data_from_lat_csv(df,lat_list[pole])\n",
    "print(\"Selecting data for bikes station on latitude \"+str(lat_list[pole]))\n",
    "#print(lat_list)\n",
    "print(df.shape)\n",
    "\n",
    "#choose one station to test on\n",
    "#df=df[df['device_name']=='CB1143']\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove outliers (bike_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14422, 48)\n",
      "We have removed 855 outliers tuples (which is 5.596648556653793 % of total).\n"
     ]
    }
   ],
   "source": [
    "def remove_outlier(df_in, col_name):\n",
    "    \"\"\"Removes all outliers on a specific column from a given dataframe.\n",
    "\n",
    "    Args:\n",
    "        df_in (pandas.DataFrame): Iput pandas dataframe containing outliers\n",
    "        col_name (str): Column name on which to search outliers\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame without outliers\n",
    "    \"\"\"         \n",
    "    q1 = df_in[col_name].quantile(0.25)\n",
    "    q3 = df_in[col_name].quantile(0.75)\n",
    "    iqr = q3-q1  # Interquartile range\n",
    "    fence_low = q1-1.5*iqr\n",
    "    fence_high = q3+1.5*iqr\n",
    "    return df_in.loc[(df_in[col_name] > fence_low) & (df_in[col_name] < fence_high)] \n",
    "start_size=df.shape[0]\n",
    "df = remove_outlier(df, \"bike_count\")\n",
    "print(df.shape)\n",
    "print(\"We have removed \"+str(start_size-df.shape[0])+\" outliers tuples (which is \"+str((start_size-df.shape[0])/start_size*100)+\" % of total).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       timestamp_from  bike_count\n",
      "62516      1544095800           3\n",
      "62517      1544096700          23\n",
      "62518      1544097600          23\n",
      "62519      1544098500          12\n",
      "62520      1544099400          13\n",
      "Index(['timestamp_from', 'bike_count'], dtype='object')\n",
      "(14422, 2)\n"
     ]
    }
   ],
   "source": [
    "def remove_features(df):\n",
    "    return df.drop(columns=[        \n",
    "    'device_name',\n",
    "    'timestamp_until',\n",
    "    'bike_avg_speed',\n",
    "    'weather_timestamp',\n",
    "    'wind_direction',\n",
    "    'wind_speed',\n",
    "    'barometer',\n",
    "    'visibility',\n",
    "    'Ice fog',\n",
    "    'Thundershowers',\n",
    "    'Sprinkles',\n",
    "    'Broken clouds',\n",
    "    'Rain showers',\n",
    "    'Snow flurries',\n",
    "    'Light fog',\n",
    "    'Sleet',\n",
    "    'Cloudy',\n",
    "    'Quite cool'    \n",
    "])\n",
    "def remove__weather_features(df):\n",
    "    return df.drop(columns=[        \n",
    "    'temperature',\n",
    "    'humidity',\n",
    "    'Scattered showers',\n",
    "    'Low clouds',\n",
    "    'Snow',\n",
    "    'Snow showers',\n",
    "    'Thunderstorms',\n",
    "    'Partly sunny',\n",
    "    'Light freezing rain',\n",
    "    'Sunny',\n",
    "    'Light rain',\n",
    "    'Freezing rain',\n",
    "    'Light snow',\n",
    "    'Passing clouds',\n",
    "    'Fog',\n",
    "    'Cool',\n",
    "    'Partly cloudy',\n",
    "    'Haze',\n",
    "    'Hail',\n",
    "    'Scattered clouds',\n",
    "    'Drizzle',\n",
    "    'Clear',\n",
    "    'Rain',\n",
    "    'Chilly'    \n",
    "        \n",
    "])\n",
    "def remove__all_features_for_ARIMA_Model(df):\n",
    "    return df.drop(columns=[        \n",
    "    'temperature',\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'time_window',\n",
    "    'time_day_of_week_window',\n",
    "    'humidity',\n",
    "    'Scattered showers',\n",
    "    'Low clouds',\n",
    "    'Snow',\n",
    "    'Snow showers',\n",
    "    'Thunderstorms',\n",
    "    'Partly sunny',\n",
    "    'Light freezing rain',\n",
    "    'Sunny',\n",
    "    'Light rain',\n",
    "    'Freezing rain',\n",
    "    'Light snow',\n",
    "    'Passing clouds',\n",
    "    'Fog',\n",
    "    'Cool',\n",
    "    'Partly cloudy',\n",
    "    'Haze',\n",
    "    'Hail',\n",
    "    'Scattered clouds',\n",
    "    'Drizzle',\n",
    "    'Clear',\n",
    "    'Rain',\n",
    "    'Chilly' \n",
    "        \n",
    "])\n",
    "\n",
    "df=remove_features(df)\n",
    "df=remove__all_features_for_ARIMA_Model(df)\n",
    "#df=remove__weather_features(df)\n",
    "print(df.head())\n",
    "print(df.columns)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating ARIMA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_data_series(data_series):\n",
    "    if isinstance(data_series,pd.core.series.Series):        \n",
    "        data_series.plot(figsize=(80,15))\n",
    "        pyplot.show()\n",
    "    else:        \n",
    "        timestamp_list=[]\n",
    "        for i in df.timestamp_from.values:     \n",
    "            timestamp_list.append(datetime.fromtimestamp(int(i)))               \n",
    "        data=pd.Series(df.bike_count.values,index=timestamp_list)        \n",
    "        data.plot(figsize=(80,15))\n",
    "        pyplot.show()     \n",
    "def plot_prediction_results(pred, df, size):\n",
    "    timestamp_list=[]\n",
    "    new_pred=[]\n",
    "    for i in pred:\n",
    "        new_pred.append(float(i))\n",
    "    print(new_pred)\n",
    "    input()\n",
    "    for i in df.timestamp_from.values[size:]:     \n",
    "        timestamp_list.append(datetime.fromtimestamp(int(i)))\n",
    "    data=pd.Series(new_pred,df.bike_count.values[size:],index=timestamp_list)\n",
    "    print(data)\n",
    "    print(type(pred[0]))\n",
    "    data.plot(figsize=(80,15))\n",
    "    pyplot.show() \n",
    "def ARIMA_grid_search(df):\n",
    "    p_grid_search_values_list=[5,8,10]\n",
    "    grid_search_values_list=[0,1,2,3,4]  \n",
    "    best=1000\n",
    "    string_best=\"\"\n",
    "    for p in p_grid_search_values_list:\n",
    "        for q in grid_search_values_list:\n",
    "            for d in grid_search_values_list:                       \n",
    "                print(\"Calculating grid search scores...\")\n",
    "                print('p',p)\n",
    "                print('q',q)\n",
    "                print('d',d)\n",
    "                X = df.bike_count.values         \n",
    "                size = int(len(X) * 0.999)\n",
    "                train, test = X[0:size], X[size:len(X)]\n",
    "                history = [x for x in train]\n",
    "                predictions = list()\n",
    "                for t in range(len(test)):                    \n",
    "                    model = ARIMA(history, order=(p,q,d))\n",
    "                    model_fit = model.fit(disp=0)\n",
    "                    output = model_fit.forecast()\n",
    "                    yhat = output[0]\n",
    "                    predictions.append(yhat)\n",
    "                    obs = test[t]\n",
    "                    history.append(obs)          \n",
    "                error = mean_squared_error(test, predictions)\n",
    "                mae=mean_absolute_error(test,predictions)\n",
    "                if math.sqrt(error)<best:\n",
    "                    print(\"New best\",math.sqrt(error))\n",
    "                    best=math.sqrt(error)\n",
    "                    string_best=\"p: \"+str(p)+\" q: \"+str(q)+\"d: \"+str(d)\n",
    "                print(\"Test RMSE:  \"+str(math.sqrt(error)))\n",
    "                print('Test MAE :  ' + str(mae))\n",
    "                print('Test MSE : %.3f' % error)              \n",
    "                pyplot.rcParams['figure.figsize'] = [80, 15]       \n",
    "                pyplot.plot(test,color='green')\n",
    "                pyplot.plot(predictions, color='red')\n",
    "                pyplot.legend(['Ground truth','Predictions'],prop={'size': 60})\n",
    "                pyplot.show()\n",
    "                print(\"XXXXXXXXXXXXXXXXX**********XXXXXXXXXXXXXXXX\")\n",
    "    print(\"best is \",best)\n",
    "    print(string_best)    \n",
    "def ARIMA_bikes(): \n",
    "    def parser(x):        \n",
    "        return datetime.fromtimestamp(int(x))\n",
    "#     series = read_csv('point_50.82448.tsv',sep='\\t', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)    \n",
    "    count=0    \n",
    "    plot_data_series(df)\n",
    "    X = df.bike_count.values         \n",
    "    size = int(len(X) * 0.80)\n",
    "    train, test = X[0:size], X[size:len(X)]\n",
    "    history = [x for x in train]\n",
    "    predictions = list()    \n",
    "#     ARIMA_grid_search(df)    \n",
    "    print(\"Calculating scores...\")\n",
    "    for t in range(len(test)):\n",
    "        model = ARIMA(history, order=(4,1,0))\n",
    "        model_fit = model.fit(disp=0)\n",
    "        output = model_fit.forecast()\n",
    "        yhat = output[0]\n",
    "        predictions.append(yhat)\n",
    "        obs = test[t]\n",
    "        history.append(obs)\n",
    "#         print(count)\n",
    "#         count+=1\n",
    "#         print('predicted=%f, expected=%f' % (yhat, obs))        \n",
    "    # del predictions[0]\t\n",
    "    # predictions.append(np.array([test[-1]]))\n",
    "    error = mean_squared_error(test, predictions)\n",
    "    mae=mean_absolute_error(test,predictions)\n",
    "    print(\"Test RMSE:  \"+str(math.sqrt(error)))\n",
    "    print('Test MAE :  ' + str(mae))\n",
    "    print('Test MSE : %.3f' % error)\n",
    "\n",
    "    pyplot.rcParams['figure.figsize'] = [80, 15]       \n",
    "    pyplot.plot(test,color='green')\n",
    "    pyplot.plot(predictions, color='red')\n",
    "    pyplot.legend(['Ground truth','Predictions'],prop={'size': 60})\n",
    "    pyplot.show()\n",
    "#     plot_prediction_results(predictions,df,size)\n",
    "# ARIMA_bikes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0718 16:50:57.511955 4557641152 deprecation_wrapper.py:119] From /Users/mactoweretro/deep_learning/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0718 16:50:57.526263 4557641152 deprecation_wrapper.py:119] From /Users/mactoweretro/deep_learning/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0718 16:50:57.528622 4557641152 deprecation_wrapper.py:119] From /Users/mactoweretro/deep_learning/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9  9  1  8 10]\n",
      "[9, 9, 1, 8, 10, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0718 16:50:57.711365 4557641152 deprecation_wrapper.py:119] From /Users/mactoweretro/deep_learning/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0718 16:50:57.958177 4557641152 deprecation.py:323] From /Users/mactoweretro/deep_learning/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0718 16:50:58.387176 4557641152 deprecation_wrapper.py:119] From /Users/mactoweretro/deep_learning/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0718 16:50:58.458893 4557641152 deprecation_wrapper.py:119] From /Users/mactoweretro/deep_learning/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.948579]]\n"
     ]
    }
   ],
   "source": [
    "# univariate lstm example\n",
    "df=df.reset_index(drop=True)\n",
    "\n",
    "raw_seq=df.bike_count.tolist()[:1000]\n",
    "x_input=array(df.bike_count.tolist()[1000:1005])\n",
    "print(x_input)\n",
    "print(df.bike_count.tolist()[1000:1006])\n",
    "# input()\n",
    "# df=df.drop(columns=['timestamp_from'])\n",
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the sequence\n",
    "        if end_ix > len(sequence)-1:\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)\n",
    "\n",
    "# define input sequence\n",
    "# raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "\n",
    "# choose a number of time steps\n",
    "n_steps = 5\n",
    "# split into samples\n",
    "X, y = split_sequence(raw_seq, n_steps)\n",
    "\n",
    "\n",
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "n_features = 1\n",
    "X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit model\n",
    "model.fit(X, y, epochs=200, verbose=0)\n",
    "# demonstrate prediction\n",
    "# x_input = array([70, 80, 90])\n",
    "x_input = x_input.reshape((1, n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:34.441047\n"
     ]
    }
   ],
   "source": [
    " print(dt.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
