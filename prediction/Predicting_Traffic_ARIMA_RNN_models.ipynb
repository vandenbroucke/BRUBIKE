{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import xgboost as xgb\n",
    "import xgboost\n",
    "import tensorflow\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras import regularizers\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import LSTM\n",
    "from tensorflow.python.keras import initializers\n",
    "from datetime import datetime as dt\n",
    "from utils import *\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from features_engineering import TIME_SERIES_FEATURES_ENGINEERING\n",
    "import sklearn\n",
    "start=dt.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process weathertypes, remove empty windspeed and store (ignore if it's already done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you want to generate the one_hot file uncomment the last line of this cell\n",
    "def update_one_hot_data(combined_path,weather_combined_path):\n",
    "    if not check_if_the_tsv_hot_file_already_has_processed_columns(weather_combined_path):        \n",
    "        \n",
    "        df = pd.read_csv(combined_path,\n",
    "                         sep='\\t',\n",
    "                         header=0) \n",
    "        weather_unique_combinations = df.weather_condition.unique()\n",
    "\n",
    "        #Get list of all unique weather types\n",
    "        types = []\n",
    "        for el in weather_unique_combinations:\n",
    "            for wc in el.split('.'):\n",
    "                if(wc != ''):            \n",
    "                    types.append(wc.strip())\n",
    "        true_unique =  set(types)\n",
    "\n",
    "        #Add columns with default value 0 for all unique weather types\n",
    "        for unique_weather_type in true_unique:\n",
    "            df[unique_weather_type]=0\n",
    "\n",
    "        #Loop over all records and set value to 1 for their corresponding weather_types\n",
    "        for index, row in df.iterrows():\n",
    "            row_types = []\n",
    "            for wc in row[\"weather_condition\"].split('.'):\n",
    "                if(wc != ''):            \n",
    "                    row_types.append(wc.strip())\n",
    "            for t in row_types:\n",
    "                df.at[index,t]=1\n",
    "\n",
    "\n",
    "        #remove empty windspeeds\n",
    "        df = df[df.wind_speed.apply(lambda x: str(x).isnumeric())]\n",
    "\n",
    "        #remove original weather_condition column and store, to avoid rerun\n",
    "        df =  df.drop(columns=\"weather_condition\")\n",
    "        df.to_csv(weather_combined_path,\n",
    "                  sep='\\t',\n",
    "                  index=False,\n",
    "                  header=True) \n",
    "        TIME_SERIES_FEATURES_ENGINEERING(weather_combined_path)\n",
    "    else:\n",
    "        print(\"File already processed.\")\n",
    "        \n",
    "combined_path = \".././data/combined_data.tsv\"\n",
    "weather_combined_path=\".././data/combined_one_hot_data.tsv\"\n",
    "#update_one_hot_data(combined_path,weather_combined_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loadin data with one_hot weather types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101461, 48)\n",
      "(101461, 48)\n"
     ]
    }
   ],
   "source": [
    "def select_data_from_lat_csv(data,latitude):\n",
    "\n",
    "    return data[data['latitude']==latitude]\n",
    "def obtain_latitudes_list(latitude_data):\n",
    "    return latitude_data.unique()\n",
    "weather_combined_path=\".././data/combined_one_hot_data.tsv\"\n",
    "df = pd.read_csv(weather_combined_path,\n",
    "                 sep='\\t',\n",
    "                 header=0)\n",
    "print(df.shape)\n",
    "lat_list=obtain_latitudes_list(df.latitude)\n",
    "pole=5\n",
    "# df=select_data_from_lat_csv(df,lat_list[pole])\n",
    "# print(\"Selecting data for bikes station on latitude \"+str(lat_list[pole]))\n",
    "#print(lat_list)\n",
    "print(df.shape)\n",
    "\n",
    "#choose one station to test on\n",
    "#df=df[df['device_name']=='CB1143']\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove outliers (bike_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(94342, 48)\n",
      "We have removed 7119 outliers tuples (which is 7.016489094331813 % of total).\n"
     ]
    }
   ],
   "source": [
    "def remove_outlier(df_in, col_name):\n",
    "    \"\"\"Removes all outliers on a specific column from a given dataframe.\n",
    "\n",
    "    Args:\n",
    "        df_in (pandas.DataFrame): Iput pandas dataframe containing outliers\n",
    "        col_name (str): Column name on which to search outliers\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame without outliers\n",
    "    \"\"\"         \n",
    "    q1 = df_in[col_name].quantile(0.25)\n",
    "    q3 = df_in[col_name].quantile(0.75)\n",
    "    iqr = q3-q1  # Interquartile range\n",
    "    fence_low = q1-1.5*iqr\n",
    "    fence_high = q3+1.5*iqr\n",
    "    return df_in.loc[(df_in[col_name] > fence_low) & (df_in[col_name] < fence_high)] \n",
    "start_size=df.shape[0]\n",
    "df = remove_outlier(df, \"bike_count\")\n",
    "print(df.shape)\n",
    "print(\"We have removed \"+str(start_size-df.shape[0])+\" outliers tuples (which is \"+str((start_size-df.shape[0])/start_size*100)+\" % of total).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   timestamp_from  bike_count\n",
      "0      1544095800           3\n",
      "1      1544096700           9\n",
      "2      1544097600          10\n",
      "3      1544098500          10\n",
      "4      1544099400           3\n",
      "Index(['timestamp_from', 'bike_count'], dtype='object')\n",
      "(94342, 2)\n"
     ]
    }
   ],
   "source": [
    "def remove_features(df):\n",
    "    return df.drop(columns=[        \n",
    "    'device_name',\n",
    "    'timestamp_until',\n",
    "    'bike_avg_speed',\n",
    "    'weather_timestamp',\n",
    "    'wind_direction',\n",
    "    'wind_speed',\n",
    "    'barometer',\n",
    "    'visibility',\n",
    "    'Ice fog',\n",
    "    'Thundershowers',\n",
    "    'Sprinkles',\n",
    "    'Broken clouds',\n",
    "    'Rain showers',\n",
    "    'Snow flurries',\n",
    "    'Light fog',\n",
    "    'Sleet',\n",
    "    'Cloudy',\n",
    "    'Quite cool'    \n",
    "])\n",
    "def remove__weather_features(df):\n",
    "    return df.drop(columns=[        \n",
    "    'temperature',\n",
    "    'humidity',\n",
    "    'Scattered showers',\n",
    "    'Low clouds',\n",
    "    'Snow',\n",
    "    'Snow showers',\n",
    "    'Thunderstorms',\n",
    "    'Partly sunny',\n",
    "    'Light freezing rain',\n",
    "    'Sunny',\n",
    "    'Light rain',\n",
    "    'Freezing rain',\n",
    "    'Light snow',\n",
    "    'Passing clouds',\n",
    "    'Fog',\n",
    "    'Cool',\n",
    "    'Partly cloudy',\n",
    "    'Haze',\n",
    "    'Hail',\n",
    "    'Scattered clouds',\n",
    "    'Drizzle',\n",
    "    'Clear',\n",
    "    'Rain',\n",
    "    'Chilly'    \n",
    "        \n",
    "])\n",
    "def remove__all_features_for_ARIMA_Model(df):\n",
    "    return df.drop(columns=[        \n",
    "    'temperature',\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'time_window',\n",
    "    'time_day_of_week_window',\n",
    "    'humidity',\n",
    "    'Scattered showers',\n",
    "    'Low clouds',\n",
    "    'Snow',\n",
    "    'Snow showers',\n",
    "    'Thunderstorms',\n",
    "    'Partly sunny',\n",
    "    'Light freezing rain',\n",
    "    'Sunny',\n",
    "    'Light rain',\n",
    "    'Freezing rain',\n",
    "    'Light snow',\n",
    "    'Passing clouds',\n",
    "    'Fog',\n",
    "    'Cool',\n",
    "    'Partly cloudy',\n",
    "    'Haze',\n",
    "    'Hail',\n",
    "    'Scattered clouds',\n",
    "    'Drizzle',\n",
    "    'Clear',\n",
    "    'Rain',\n",
    "    'Chilly' \n",
    "        \n",
    "])\n",
    "\n",
    "df=remove_features(df)\n",
    "df=remove__all_features_for_ARIMA_Model(df)\n",
    "#df=remove__weather_features(df)\n",
    "print(df.head())\n",
    "print(df.columns)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating ARIMA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        timestamp_from  bike_count\n",
      "0           1544095800           3\n",
      "1           1544096700           9\n",
      "2           1544097600          10\n",
      "3           1544098500          10\n",
      "4           1544099400           3\n",
      "5           1544445900          11\n",
      "6           1544446800          16\n",
      "7           1544447700           9\n",
      "8           1544448600          16\n",
      "9           1544449500           5\n",
      "10          1544510700          14\n",
      "12          1544512500          22\n",
      "13          1544513400          15\n",
      "14          1544514300          14\n",
      "15          1544515200          17\n",
      "16          1544516100           6\n",
      "17          1544517000           4\n",
      "18          1544517900           3\n",
      "19          1544518800           8\n",
      "20          1544519700          10\n",
      "21          1544520600           9\n",
      "22          1544521500          11\n",
      "23          1544522400           9\n",
      "24          1544523300          10\n",
      "25          1544524200          10\n",
      "26          1544525100          13\n",
      "27          1544526000           9\n",
      "28          1544526900           6\n",
      "29          1544527800          14\n",
      "30          1544528700          13\n",
      "...                ...         ...\n",
      "101431      1551995100           1\n",
      "101432      1551996000           3\n",
      "101433      1551996900           2\n",
      "101434      1551997800           2\n",
      "101435      1551998700           1\n",
      "101436      1552000500           2\n",
      "101437      1552002300           2\n",
      "101438      1552004100           3\n",
      "101439      1552005000           2\n",
      "101440      1552005900           2\n",
      "101441      1552007700           2\n",
      "101442      1552015800           1\n",
      "101443      1552017600           1\n",
      "101444      1552018500           2\n",
      "101445      1552019400           2\n",
      "101446      1552020300           1\n",
      "101447      1552021200           1\n",
      "101448      1552022100           3\n",
      "101449      1552023000           2\n",
      "101450      1552023900           6\n",
      "101451      1552024800           9\n",
      "101452      1552025700           8\n",
      "101453      1552026600          11\n",
      "101454      1552027500          14\n",
      "101455      1552028400           9\n",
      "101456      1552029300          10\n",
      "101457      1552030200           8\n",
      "101458      1552031100           9\n",
      "101459      1552032000           4\n",
      "101460      1552032900           4\n",
      "\n",
      "[94342 rows x 2 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "timestamp_until\n",
      "2018-12-06 12:45:00     3\n",
      "2018-12-06 13:00:00     9\n",
      "2018-12-06 13:15:00    10\n",
      "2018-12-06 13:30:00    10\n",
      "2018-12-06 13:45:00     3\n",
      "2018-12-10 14:00:00    11\n",
      "2018-12-10 14:15:00    16\n",
      "2018-12-10 14:30:00     9\n",
      "2018-12-10 14:45:00    16\n",
      "2018-12-10 15:00:00     5\n",
      "2018-12-11 08:00:00    14\n",
      "2018-12-11 08:15:00    32\n",
      "2018-12-11 08:30:00    22\n",
      "2018-12-11 08:45:00    15\n",
      "2018-12-11 09:00:00    14\n",
      "2018-12-11 09:15:00    17\n",
      "2018-12-11 09:30:00     6\n",
      "2018-12-11 09:45:00     4\n",
      "2018-12-11 10:00:00     3\n",
      "2018-12-11 10:15:00     8\n",
      "2018-12-11 10:30:00    10\n",
      "2018-12-11 10:45:00     9\n",
      "2018-12-11 11:00:00    11\n",
      "2018-12-11 11:15:00     9\n",
      "2018-12-11 11:30:00    10\n",
      "2018-12-11 11:45:00    10\n",
      "2018-12-11 12:00:00    13\n",
      "2018-12-11 12:15:00     9\n",
      "2018-12-11 12:30:00     6\n",
      "2018-12-11 12:45:00    14\n",
      "                       ..\n",
      "2019-06-16 16:45:00     9\n",
      "2019-06-16 17:00:00    14\n",
      "2019-06-16 17:15:00     8\n",
      "2019-06-16 17:30:00     8\n",
      "2019-06-16 17:45:00     8\n",
      "2019-06-16 18:00:00     5\n",
      "2019-06-16 18:15:00     9\n",
      "2019-06-16 18:30:00     5\n",
      "2019-06-16 18:45:00     5\n",
      "2019-06-16 19:00:00    11\n",
      "2019-06-16 19:15:00    13\n",
      "2019-06-16 19:30:00     6\n",
      "2019-06-16 19:45:00    10\n",
      "2019-06-16 20:00:00     6\n",
      "2019-06-16 20:15:00     5\n",
      "2019-06-16 20:30:00    10\n",
      "2019-06-16 20:45:00     3\n",
      "2019-06-16 21:00:00     3\n",
      "2019-06-16 21:15:00     3\n",
      "2019-06-16 21:30:00     3\n",
      "2019-06-16 21:45:00     2\n",
      "2019-06-16 22:00:00     1\n",
      "2019-06-16 22:15:00     5\n",
      "2019-06-16 22:30:00     2\n",
      "2019-06-16 22:45:00     2\n",
      "2019-06-16 23:00:00     3\n",
      "2019-06-16 23:15:00     2\n",
      "2019-06-16 23:30:00     1\n",
      "2019-06-16 23:45:00     2\n",
      "2019-06-16 23:59:59     3\n",
      "Name: bike_count, Length: 14235, dtype: int64\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "def ARIMA_bikes():\n",
    "    from pandas import read_csv\n",
    "    from pandas import datetime\n",
    "    from matplotlib import pyplot\n",
    "    from statsmodels.tsa.arima_model import ARIMA\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    import numpy as np\n",
    "    import math\n",
    "\n",
    "    def parser(x):\n",
    "        # print(datetime.fromtimestamp(int(x)))\n",
    "        # print(type(datetime.fromtimestamp(int(x))))\n",
    "        # input()\n",
    "        return datetime.fromtimestamp(int(x))\n",
    "\n",
    "    series = read_csv('point_50.82448.tsv',sep='\\t', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)\n",
    "    print(df)\n",
    "    print(type(df))\n",
    "    print(series)\n",
    "    print(type(series))\n",
    "    input()\n",
    "    # print(series.head())\n",
    "    # series.plot()\n",
    "    # pyplot.show()\n",
    "    # print(series.values)\n",
    "    # print(type(series.values))\n",
    "    # input()\n",
    "    count=0\n",
    "    X = series.values\n",
    "    size = int(len(X) * 0.998)\n",
    "    train, test = X[0:size], X[size:len(X)]\n",
    "    history = [x for x in train]\n",
    "    predictions = list()\n",
    "    for t in range(len(test)):\n",
    "        model = ARIMA(history, order=(5,1,0))\n",
    "        model_fit = model.fit(disp=0)\n",
    "        output = model_fit.forecast()\n",
    "        yhat = output[0]\n",
    "        predictions.append(yhat)\n",
    "        obs = test[t]\n",
    "        history.append(obs)\n",
    "        print(count)\n",
    "        count+=1\n",
    "        print('predicted=%f, expected=%f' % (yhat, obs))\n",
    "    # del predictions[0]\t\n",
    "    # predictions.append(np.array([test[-1]]))\n",
    "    error = mean_squared_error(test, predictions)\n",
    "    mae=mean_absolute_error(test,predictions)\n",
    "    print('Test MAE : %.3f' % mae)\n",
    "    print('Test MSE : %.3f' % error)\n",
    "    print(\"Test RMSE:%.3f \"%math.sqrt(error))\n",
    "    # plot\n",
    "    print(type(predictions))\n",
    "    pyplot.plot(test)\n",
    "    pyplot.plot(predictions, color='red')\n",
    "    pyplot.show()\n",
    "ARIMA_bikes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " print(dt.now()-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
