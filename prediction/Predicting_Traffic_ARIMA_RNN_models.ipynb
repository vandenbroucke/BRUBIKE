{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import xgboost as xgb\n",
    "import xgboost\n",
    "import tensorflow\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras import regularizers\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import LSTM\n",
    "from tensorflow.python.keras import initializers\n",
    "from datetime import datetime as dt\n",
    "from utils import *\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from features_engineering import TIME_SERIES_FEATURES_ENGINEERING\n",
    "import sklearn\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from matplotlib import pyplot\n",
    "from statsmodels.tsa.arima_model import ARIMA    \n",
    "import numpy as np\n",
    "import math\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "start=dt.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process weathertypes, remove empty windspeed and store (ignore if it's already done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you want to generate the one_hot file uncomment the last line of this cell\n",
    "def update_one_hot_data(combined_path,weather_combined_path):\n",
    "    if not check_if_the_tsv_hot_file_already_has_processed_columns(weather_combined_path):        \n",
    "        \n",
    "        df = pd.read_csv(combined_path,\n",
    "                         sep='\\t',\n",
    "                         header=0) \n",
    "        weather_unique_combinations = df.weather_condition.unique()\n",
    "\n",
    "        #Get list of all unique weather types\n",
    "        types = []\n",
    "        for el in weather_unique_combinations:\n",
    "            for wc in el.split('.'):\n",
    "                if(wc != ''):            \n",
    "                    types.append(wc.strip())\n",
    "        true_unique =  set(types)\n",
    "\n",
    "        #Add columns with default value 0 for all unique weather types\n",
    "        for unique_weather_type in true_unique:\n",
    "            df[unique_weather_type]=0\n",
    "\n",
    "        #Loop over all records and set value to 1 for their corresponding weather_types\n",
    "        for index, row in df.iterrows():\n",
    "            row_types = []\n",
    "            for wc in row[\"weather_condition\"].split('.'):\n",
    "                if(wc != ''):            \n",
    "                    row_types.append(wc.strip())\n",
    "            for t in row_types:\n",
    "                df.at[index,t]=1\n",
    "\n",
    "\n",
    "        #remove empty windspeeds\n",
    "        df = df[df.wind_speed.apply(lambda x: str(x).isnumeric())]\n",
    "\n",
    "        #remove original weather_condition column and store, to avoid rerun\n",
    "        df =  df.drop(columns=\"weather_condition\")\n",
    "        df.to_csv(weather_combined_path,\n",
    "                  sep='\\t',\n",
    "                  index=False,\n",
    "                  header=True) \n",
    "        TIME_SERIES_FEATURES_ENGINEERING(weather_combined_path)\n",
    "    else:\n",
    "        print(\"File already processed.\")\n",
    "        \n",
    "combined_path = \".././data/combined_data.tsv\"\n",
    "weather_combined_path=\".././data/combined_one_hot_data.tsv\"\n",
    "#update_one_hot_data(combined_path,weather_combined_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loadin data with one_hot weather types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_data_from_lat_csv(data,latitude):\n",
    "\n",
    "    return data[data['latitude']==latitude]\n",
    "def obtain_latitudes_list(latitude_data):\n",
    "    return latitude_data.unique()\n",
    "weather_combined_path=\".././data/combined_one_hot_data.tsv\"\n",
    "df = pd.read_csv(weather_combined_path,\n",
    "                 sep='\\t',\n",
    "                 header=0)\n",
    "print(df.shape)\n",
    "lat_list=obtain_latitudes_list(df.latitude)\n",
    "pole=5\n",
    "df=select_data_from_lat_csv(df,lat_list[pole])\n",
    "print(\"Selecting data for bikes station on latitude \"+str(lat_list[pole]))\n",
    "#print(lat_list)\n",
    "print(df.shape)\n",
    "\n",
    "#choose one station to test on\n",
    "#df=df[df['device_name']=='CB1143']\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove outliers (bike_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outlier(df_in, col_name):\n",
    "    \"\"\"Removes all outliers on a specific column from a given dataframe.\n",
    "\n",
    "    Args:\n",
    "        df_in (pandas.DataFrame): Iput pandas dataframe containing outliers\n",
    "        col_name (str): Column name on which to search outliers\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame without outliers\n",
    "    \"\"\"         \n",
    "    q1 = df_in[col_name].quantile(0.25)\n",
    "    q3 = df_in[col_name].quantile(0.75)\n",
    "    iqr = q3-q1  # Interquartile range\n",
    "    fence_low = q1-1.5*iqr\n",
    "    fence_high = q3+1.5*iqr\n",
    "    return df_in.loc[(df_in[col_name] > fence_low) & (df_in[col_name] < fence_high)] \n",
    "start_size=df.shape[0]\n",
    "df = remove_outlier(df, \"bike_count\")\n",
    "print(df.shape)\n",
    "print(\"We have removed \"+str(start_size-df.shape[0])+\" outliers tuples (which is \"+str((start_size-df.shape[0])/start_size*100)+\" % of total).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_features(df):\n",
    "    return df.drop(columns=[        \n",
    "    'device_name',\n",
    "    'timestamp_until',\n",
    "    'bike_avg_speed',\n",
    "    'weather_timestamp',\n",
    "    'wind_direction',\n",
    "    'wind_speed',\n",
    "    'barometer',\n",
    "    'visibility',\n",
    "    'Ice fog',\n",
    "    'Thundershowers',\n",
    "    'Sprinkles',\n",
    "    'Broken clouds',\n",
    "    'Rain showers',\n",
    "    'Snow flurries',\n",
    "    'Light fog',\n",
    "    'Sleet',\n",
    "    'Cloudy',\n",
    "    'Quite cool'    \n",
    "])\n",
    "def remove__weather_features(df):\n",
    "    return df.drop(columns=[        \n",
    "    'temperature',\n",
    "    'humidity',\n",
    "    'Scattered showers',\n",
    "    'Low clouds',\n",
    "    'Snow',\n",
    "    'Snow showers',\n",
    "    'Thunderstorms',\n",
    "    'Partly sunny',\n",
    "    'Light freezing rain',\n",
    "    'Sunny',\n",
    "    'Light rain',\n",
    "    'Freezing rain',\n",
    "    'Light snow',\n",
    "    'Passing clouds',\n",
    "    'Fog',\n",
    "    'Cool',\n",
    "    'Partly cloudy',\n",
    "    'Haze',\n",
    "    'Hail',\n",
    "    'Scattered clouds',\n",
    "    'Drizzle',\n",
    "    'Clear',\n",
    "    'Rain',\n",
    "    'Chilly'    \n",
    "        \n",
    "])\n",
    "def remove__all_features_for_ARIMA_Model(df):\n",
    "    return df.drop(columns=[        \n",
    "    'temperature',\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'time_window',\n",
    "    'time_day_of_week_window',\n",
    "    'humidity',\n",
    "    'Scattered showers',\n",
    "    'Low clouds',\n",
    "    'Snow',\n",
    "    'Snow showers',\n",
    "    'Thunderstorms',\n",
    "    'Partly sunny',\n",
    "    'Light freezing rain',\n",
    "    'Sunny',\n",
    "    'Light rain',\n",
    "    'Freezing rain',\n",
    "    'Light snow',\n",
    "    'Passing clouds',\n",
    "    'Fog',\n",
    "    'Cool',\n",
    "    'Partly cloudy',\n",
    "    'Haze',\n",
    "    'Hail',\n",
    "    'Scattered clouds',\n",
    "    'Drizzle',\n",
    "    'Clear',\n",
    "    'Rain',\n",
    "    'Chilly' \n",
    "        \n",
    "])\n",
    "\n",
    "df=remove_features(df)\n",
    "df=remove__all_features_for_ARIMA_Model(df)\n",
    "#df=remove__weather_features(df)\n",
    "print(df.head())\n",
    "print(df.columns)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating ARIMA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_data_series(data_series):\n",
    "    if isinstance(data_series,pd.core.series.Series):        \n",
    "        data_series.plot(figsize=(80,15))\n",
    "        pyplot.show()\n",
    "    else:        \n",
    "        timestamp_list=[]\n",
    "        for i in df.timestamp_from.values:     \n",
    "            timestamp_list.append(datetime.fromtimestamp(int(i)))               \n",
    "        data=pd.Series(df.bike_count.values,index=timestamp_list)        \n",
    "        data.plot(figsize=(80,15))\n",
    "        pyplot.show()     \n",
    "def plot_prediction_results(pred, df, size):\n",
    "    timestamp_list=[]\n",
    "    new_pred=[]\n",
    "    for i in pred:\n",
    "        new_pred.append(float(i))\n",
    "    print(new_pred)\n",
    "    input()\n",
    "    for i in df.timestamp_from.values[size:]:     \n",
    "        timestamp_list.append(datetime.fromtimestamp(int(i)))\n",
    "    data=pd.Series(new_pred,df.bike_count.values[size:],index=timestamp_list)\n",
    "    print(data)\n",
    "    print(type(pred[0]))\n",
    "    data.plot(figsize=(80,15))\n",
    "    pyplot.show() \n",
    "def ARIMA_grid_search(df):\n",
    "    p_grid_search_values_list=[5,8,10]\n",
    "    grid_search_values_list=[0,1,2,3,4]  \n",
    "    best=1000\n",
    "    string_best=\"\"\n",
    "    for p in p_grid_search_values_list:\n",
    "        for q in grid_search_values_list:\n",
    "            for d in grid_search_values_list:                       \n",
    "                print(\"Calculating grid search scores...\")\n",
    "                print('p',p)\n",
    "                print('q',q)\n",
    "                print('d',d)\n",
    "                X = df.bike_count.values         \n",
    "                size = int(len(X) * 0.999)\n",
    "                train, test = X[0:size], X[size:len(X)]\n",
    "                history = [x for x in train]\n",
    "                predictions = list()\n",
    "                for t in range(len(test)):                    \n",
    "                    model = ARIMA(history, order=(p,q,d))\n",
    "                    model_fit = model.fit(disp=0)\n",
    "                    output = model_fit.forecast()\n",
    "                    yhat = output[0]\n",
    "                    predictions.append(yhat)\n",
    "                    obs = test[t]\n",
    "                    history.append(obs)          \n",
    "                error = mean_squared_error(test, predictions)\n",
    "                mae=mean_absolute_error(test,predictions)\n",
    "                if math.sqrt(error)<best:\n",
    "                    print(\"New best\",math.sqrt(error))\n",
    "                    best=math.sqrt(error)\n",
    "                    string_best=\"p: \"+str(p)+\" q: \"+str(q)+\"d: \"+str(d)\n",
    "                print(\"Test RMSE:  \"+str(math.sqrt(error)))\n",
    "                print('Test MAE :  ' + str(mae))\n",
    "                print('Test MSE : %.3f' % error)              \n",
    "                pyplot.rcParams['figure.figsize'] = [80, 15]       \n",
    "                pyplot.plot(test,color='green')\n",
    "                pyplot.plot(predictions, color='red')\n",
    "                pyplot.legend(['Ground truth','Predictions'],prop={'size': 60})\n",
    "                pyplot.show()\n",
    "                print(\"XXXXXXXXXXXXXXXXX**********XXXXXXXXXXXXXXXX\")\n",
    "    print(\"best is \",best)\n",
    "    print(string_best)    \n",
    "def ARIMA_bikes(): \n",
    "    def parser(x):        \n",
    "        return datetime.fromtimestamp(int(x))\n",
    "#     series = read_csv('point_50.82448.tsv',sep='\\t', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)    \n",
    "    count=0    \n",
    "    plot_data_series(df)\n",
    "    X = df.bike_count.values         \n",
    "    size = int(len(X) * 0.80)\n",
    "    train, test = X[0:size], X[size:len(X)]\n",
    "    history = [x for x in train]\n",
    "    predictions = list()    \n",
    "#     ARIMA_grid_search(df)    \n",
    "    print(\"Calculating scores...\")\n",
    "    for t in range(len(test)):\n",
    "        model = ARIMA(history, order=(4,1,0))\n",
    "        model_fit = model.fit(disp=0)\n",
    "        output = model_fit.forecast()\n",
    "        yhat = output[0]\n",
    "        predictions.append(yhat)\n",
    "        obs = test[t]\n",
    "        history.append(obs)\n",
    "#         print(count)\n",
    "#         count+=1\n",
    "#         print('predicted=%f, expected=%f' % (yhat, obs))        \n",
    "    # del predictions[0]\t\n",
    "    # predictions.append(np.array([test[-1]]))\n",
    "    error = mean_squared_error(test, predictions)\n",
    "    mae=mean_absolute_error(test,predictions)\n",
    "    print(\"Test RMSE:  \"+str(math.sqrt(error)))\n",
    "    print('Test MAE :  ' + str(mae))\n",
    "    print('Test MSE : %.3f' % error)\n",
    "\n",
    "    pyplot.rcParams['figure.figsize'] = [80, 15]       \n",
    "    pyplot.plot(test,color='green')\n",
    "    pyplot.plot(predictions, color='red')\n",
    "    pyplot.legend(['Ground truth','Predictions'],prop={'size': 60})\n",
    "    pyplot.show()\n",
    "#     plot_prediction_results(predictions,df,size)\n",
    "# ARIMA_bikes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM (fit data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# peng ~ pong\n",
    "def configure_and_run_LSTM(window,df,size):    \n",
    "    df=df.reset_index(drop=True)  \n",
    "    trining_set=df.bike_count.tolist()[:size]      \n",
    "    def split_sequence(sequence, n_steps):\n",
    "        X, y = list(), list()\n",
    "        for i in range(len(sequence)):\n",
    "            # find the end of this pattern\n",
    "            end_ix = i + n_steps\n",
    "            # check if we are beyond the sequence\n",
    "            if end_ix > len(sequence)-1:\n",
    "                break\n",
    "            # gather input and output parts of the pattern\n",
    "            seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "            X.append(seq_x)\n",
    "            y.append(seq_y)\n",
    "        return array(X), array(y)\n",
    "    \n",
    "    # define input sequence\n",
    "    # raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "    # choose a number of time steps\n",
    "    n_steps = window\n",
    "    # split into samples\n",
    "    X, y = split_sequence(trining_set, n_steps)\n",
    "    # reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "    n_features = 1\n",
    "    X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    # fit model\n",
    "    model.fit(X, y, epochs=200, verbose=0)    \n",
    "    return model\n",
    "size = int(df.shape[0] * 0.80)\n",
    "window=8\n",
    "model_lstm=configure_and_run_LSTM(window,df,size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " print(dt.now()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM (predict data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_LSTM(model,df,size,n_steps,n_features=1):        \n",
    "    ground_T=[]\n",
    "    predictions=[]\n",
    "    for i in range(size,df.shape[0]-n_steps):    \n",
    "        x_input=array(df.bike_count.tolist()[i:i+n_steps])          \n",
    "        # demonstrate prediction\n",
    "        # x_input = array([70, 80, 90])\n",
    "        x_input = x_input.reshape((1, n_steps, n_features))\n",
    "        yhat = model.predict(x_input, verbose=0)        \n",
    "        ground_T.append(df.bike_count.tolist()[i+n_steps])\n",
    "        predictions.append(yhat[0][0])\n",
    "    MSE_error = mean_squared_error(ground_T, predictions)\n",
    "    mae=mean_absolute_error(ground_T,predictions)\n",
    "    print(\"RMSE: \"+str(math.sqrt(MSE_error)))\n",
    "    print(\"MAE:  \"+str(mae))\n",
    "predict_LSTM(model_lstm,df,size,window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
