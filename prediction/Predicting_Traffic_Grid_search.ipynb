{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import xgboost as xgb\n",
    "import xgboost\n",
    "import tensorflow\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras import regularizers\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import LSTM\n",
    "from tensorflow.python.keras import initializers\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from datetime import datetime as dt\n",
    "from utils import *\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from features_engineering import TIME_SERIES_FEATURES_ENGINEERING\n",
    "import sklearn\n",
    "start=dt.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process weathertypes, remove empty windspeed and store (ignore if it's already done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you want to generate the one_hot file uncomment the last line of this cell\n",
    "def update_one_hot_data(combined_path,weather_combined_path):\n",
    "    if not check_if_the_tsv_hot_file_already_has_processed_columns(weather_combined_path):        \n",
    "        \n",
    "        df = pd.read_csv(combined_path,\n",
    "                         sep='\\t',\n",
    "                         header=0) \n",
    "        weather_unique_combinations = df.weather_condition.unique()\n",
    "\n",
    "        #Get list of all unique weather types\n",
    "        types = []\n",
    "        for el in weather_unique_combinations:\n",
    "            for wc in el.split('.'):\n",
    "                if(wc != ''):            \n",
    "                    types.append(wc.strip())\n",
    "        true_unique =  set(types)\n",
    "\n",
    "        #Add columns with default value 0 for all unique weather types\n",
    "        for unique_weather_type in true_unique:\n",
    "            df[unique_weather_type]=0\n",
    "\n",
    "        #Loop over all records and set value to 1 for their corresponding weather_types\n",
    "        for index, row in df.iterrows():\n",
    "            row_types = []\n",
    "            for wc in row[\"weather_condition\"].split('.'):\n",
    "                if(wc != ''):            \n",
    "                    row_types.append(wc.strip())\n",
    "            for t in row_types:\n",
    "                df.at[index,t]=1\n",
    "\n",
    "\n",
    "        #remove empty windspeeds\n",
    "        df = df[df.wind_speed.apply(lambda x: str(x).isnumeric())]\n",
    "\n",
    "        #remove original weather_condition column and store, to avoid rerun\n",
    "        df =  df.drop(columns=\"weather_condition\")\n",
    "        df.to_csv(weather_combined_path,\n",
    "                  sep='\\t',\n",
    "                  index=False,\n",
    "                  header=True) \n",
    "        TIME_SERIES_FEATURES_ENGINEERING(weather_combined_path)\n",
    "    else:\n",
    "        print(\"File already processed.\")\n",
    "        \n",
    "combined_path = \".././data/combined_data.tsv\"\n",
    "weather_combined_path=\".././data/combined_one_hot_data.tsv\"\n",
    "#update_one_hot_data(combined_path,weather_combined_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loadin data with one_hot weather types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_data_from_lat_csv(data,latitude):\n",
    "\n",
    "    return data[data['latitude']==latitude]\n",
    "def obtain_latitudes_list(latitude_data):\n",
    "    return latitude_data.unique()\n",
    "weather_combined_path=\".././data/combined_one_hot_data.tsv\"\n",
    "df = pd.read_csv(weather_combined_path,\n",
    "                 sep='\\t',\n",
    "                 header=0)\n",
    "print(df.shape)\n",
    "lat_list=obtain_latitudes_list(df.latitude)\n",
    "pole=0\n",
    "#df=select_data_from_lat_csv(df,lat_list[pole])\n",
    "#print(\"Selecting data for bikes station on latitude \"+str(lat_list[pole]))\n",
    "#print(lat_list)\n",
    "print(df.shape)\n",
    "\n",
    "#choose one station to test on\n",
    "#df=df[df['device_name']=='CB1143']\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove outliers (bike_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outlier(df_in, col_name):\n",
    "    \"\"\"Removes all outliers on a specific column from a given dataframe.\n",
    "\n",
    "    Args:\n",
    "        df_in (pandas.DataFrame): Iput pandas dataframe containing outliers\n",
    "        col_name (str): Column name on which to search outliers\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame without outliers\n",
    "    \"\"\"         \n",
    "    q1 = df_in[col_name].quantile(0.25)\n",
    "    q3 = df_in[col_name].quantile(0.75)\n",
    "    iqr = q3-q1  # Interquartile range\n",
    "    fence_low = q1-1.5*iqr\n",
    "    fence_high = q3+1.5*iqr\n",
    "    return df_in.loc[(df_in[col_name] > fence_low) & (df_in[col_name] < fence_high)] \n",
    "start_size=df.shape[0]\n",
    "df = remove_outlier(df, \"bike_count\")\n",
    "print(df.shape)\n",
    "print(\"We have removed \"+str(start_size-df.shape[0])+\" outliers tuples (which is \"+str((start_size-df.shape[0])/start_size*100)+\" % of total).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation of variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(30,30))    \n",
    "\n",
    "correlation_matrix = df.corr().round(2)\n",
    "#print(correlation_matrix)\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".001f\",ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_features(df):\n",
    "    return df.drop(columns=[        \n",
    "    'device_name',\n",
    "    'timestamp_until',\n",
    "    'bike_avg_speed',\n",
    "    'weather_timestamp',\n",
    "    'wind_direction',\n",
    "    'wind_speed',\n",
    "    'barometer',\n",
    "    'visibility',\n",
    "    'Ice fog',\n",
    "    'Thundershowers',\n",
    "    'Sprinkles',\n",
    "    'Broken clouds',\n",
    "    'Rain showers',\n",
    "    'Snow flurries',\n",
    "    'Light fog',\n",
    "    'Sleet',\n",
    "    'Cloudy',\n",
    "    'Quite cool'    \n",
    "])\n",
    "def remove__weather_features(df):\n",
    "    return df.drop(columns=[        \n",
    "    'temperature',\n",
    "    'humidity',\n",
    "    'Scattered showers',\n",
    "    'Low clouds',\n",
    "    'Snow',\n",
    "    'Snow showers',\n",
    "    'Thunderstorms',\n",
    "    'Partly sunny',\n",
    "    'Light freezing rain',\n",
    "    'Sunny',\n",
    "    'Light rain',\n",
    "    'Freezing rain',\n",
    "    'Light snow',\n",
    "    'Passing clouds',\n",
    "    'Fog',\n",
    "    'Cool',\n",
    "    'Partly cloudy',\n",
    "    'Haze',\n",
    "    'Hail',\n",
    "    'Scattered clouds',\n",
    "    'Drizzle',\n",
    "    'Clear',\n",
    "    'Rain',\n",
    "    'Chilly'    \n",
    "        \n",
    "])\n",
    "\n",
    "df=remove_features(df)\n",
    "#df=remove__weather_features(df)\n",
    "#print(df.head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_normal_model():\n",
    "    #look for regularization with keras \n",
    "    initializer = initializers.VarianceScaling(scale=1.0, mode='fan_in', distribution='normal', seed=None)\n",
    "    model = Sequential()   \n",
    "    \n",
    "    model.add(Dense(28, input_dim=28,kernel_regularizer=regularizers.l2(0.0001),kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(14,activation='relu'))    \n",
    "    model.add(Dense(1, activation='relu'))\n",
    "    optimizer = opt=tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)    \n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=['mse','mae'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "def build_Random_forest_model():\n",
    "    #model = RandomForestRegressor(max_depth=2, random_state=0,n_estimators=100)\n",
    "    model = RandomForestRegressor(max_depth=7,n_estimators=8)\n",
    "    return model\n",
    "def build_SVM_REGRESSION_model():\n",
    "    model=SVR()\n",
    "    return model\n",
    "def build_XGBOOST_reg_model():\n",
    "    model = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.01,max_depth = 7, alpha = 0, n_estimators = 400)\n",
    "    return model\n",
    "normal_model = build_normal_model()\n",
    "#normal_model = build_Random_forest_model()\n",
    "#normal_model=build_SVM_REGRESSION_model()\n",
    "# normal_model=build_XGBOOST_reg_model()\n",
    "model = normal_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining model execution functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_normal_model_grid_search(input_nodes,hidden_layer_nodes,init_kernel,optimizer,loss_item,activation_item,regularizer_parameter):\n",
    "    #look for regularization with keras \n",
    "    \n",
    "    model = Sequential()   \n",
    "    \n",
    "    model.add(Dense(input_nodes, input_dim=28,kernel_regularizer=regularizers.l2(regularizer_parameter),kernel_initializer=init_kernel, activation=activation_item))\n",
    "    model.add(Dense(hidden_layer_nodes,activation=activation_item))    \n",
    "    model.add(Dense(1, activation=activation_item))    \n",
    "    model.compile(loss=loss_item, optimizer=optimizer, metrics=['mse','mae'])\n",
    "    model.summary()\n",
    "    return model\n",
    "def gridSearch_neural_network(X_train, y_train,baseline_model):\n",
    "    input_layer_nodes=[28]\n",
    "    hidden_layer_nodes=[14]\n",
    "    glorot='glorot_uniform'\n",
    "    init = ['normal']\n",
    "    RMSprop = tf.keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "    SGD=tf.keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)    \n",
    "    Adam=tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)    \n",
    "    optimizer_list=[Adam]\n",
    "    \n",
    "    mean_squared_error=\"mean_squared_error\"\n",
    "    mean_absolute_error=\"mean_absolute_error\"         \n",
    "    hinge=\"hinge\"\n",
    "    \n",
    "    losses_f_list=[mean_squared_error]\n",
    "        \n",
    "    activation_list=['relu']\n",
    "    \n",
    "    num_inodes=input_layer_nodes[0]\n",
    "    num_hnodes=hidden_layer_nodes[0]\n",
    "    init_kernel_name=init[0]\n",
    "    optimizer_alg=optimizer_list[0]\n",
    "    loss_function_f=losses_f_list[0]\n",
    "    activation_f=activation_list[0]\n",
    "    best_performance=1000\n",
    "    \n",
    "    learning_rate_combinations_list=[0.0001,0.0003,0.001,0.003,0.01,0.03]\n",
    "    b_learning_rate=learning_rate_combinations_list[0]\n",
    "    b_regularization_rate_parameter=learning_rate_combinations_list[0]\n",
    "    \n",
    "    historical_performance_list=[]\n",
    "    counter=0\n",
    "    for inodes in input_layer_nodes:\n",
    "        for hnodes in hidden_layer_nodes:\n",
    "            for init_k in init:\n",
    "                for opt in optimizer_list:\n",
    "                    for loss_item in losses_f_list:\n",
    "                        for activation_item in activation_list:   \n",
    "                            for learning_rate in learning_rate_combinations_list:\n",
    "                                opt=tf.keras.optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)    \n",
    "                                for reg_parameter in learning_rate_combinations_list:\n",
    "                                    for i in range(0,1):\n",
    "                                        baseline_model=build_normal_model_grid_search(inodes,hnodes,init_k,opt,loss_item,activation_item,reg_parameter)\n",
    "                                        history=baseline_model.fit(X_train, y_train,epochs=100,verbose=0)\n",
    "                                        counter+=1\n",
    "\n",
    "                                        if best_performance > history.history['mean_squared_error'][len(history.history['mean_squared_error'])-1]:\n",
    "                                            best_performance = history.history['mean_squared_error'][len(history.history['mean_squared_error'])-1]\n",
    "                                            num_inodes=inodes\n",
    "                                            num_hnodes=hnodes\n",
    "                                            init_kernel_name=init_k\n",
    "                                            optimizer_alg=opt\n",
    "                                            loss_function_f=loss_item\n",
    "                                            activation_f=activation_item\n",
    "                                            b_learning_rate=learning_rate\n",
    "                                            b_regularization_rate_parameter=reg_parameter\n",
    "                                            print(\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\")\n",
    "                                            print(history.history['mean_squared_error'])\n",
    "                                            print(\"We have a new BEST PERFORMANCE: \"+str(best_performance))\n",
    "                                            print(\"num input nodes \"+str(num_inodes))\n",
    "                                            print(\"num idden layer nodes: \"+str(num_hnodes))\n",
    "                                            print(\"init kernel name: \"+str(init_kernel_name))\n",
    "                                            print(\"optimizer algorith: \"+str(optimizer_alg))\n",
    "                                            print(\"loss function: \"+str(loss_function_f))\n",
    "                                            print(\"activation function: \"+str(activation_f))\n",
    "                                            print(\"b_learning_rate: \"+str(b_learning_rate))\n",
    "                                            print(\"b_regularization_rate_parameter: \"+str(b_regularization_rate_parameter))\n",
    "                                            print(\"COUNTER= \"+str(counter))\n",
    "                                            historical_performance_list.append([counter,best_performance])\n",
    "                                            print(\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\")\n",
    "\n",
    "                    \n",
    "    print(\"*****************************************************\")\n",
    "    print(\"FINALLY OUR BEST PERFORMANCE: \"+str(best_performance))\n",
    "    print(num_inodes)\n",
    "    print(num_hnodes)\n",
    "    print(init_kernel_name)\n",
    "    print(optimizer_alg)\n",
    "    print(loss_function_f)\n",
    "    print(activation_f)\n",
    "    print(\"***************************************************************************\")\n",
    "    print(\"historical_performance_list:\")\n",
    "    print(historical_performance_list)\n",
    "    print(\"***************************************************************************\")\n",
    "def build_Random_fores_model_grid_search(estimator,depth):\n",
    "    return RandomForestRegressor(max_depth=depth,n_estimators=estimator)\n",
    "    \n",
    "def gridSearch_Random_forest(x_in, y_in,baseline_model):\n",
    "    test_size = 0.2\n",
    "    total = x_in.shape[0]\n",
    "    train_idx = round((1-test_size) * total)\n",
    "\n",
    "\n",
    "\n",
    "    x_train = x_in[0:train_idx]\n",
    "    #x_train = x_train.reshape(x_train.shape[0], 1, x_train.shape[1])\n",
    "\n",
    "    x_test = x_in[train_idx+1:total-1]\n",
    "    #x_test = x_test.reshape(x_test.shape[0], 1, x_test.shape[1])\n",
    "\n",
    "    y_train = y_in[0:train_idx]\n",
    "    y_test = y_in[train_idx+1:total-1]\n",
    "\n",
    "    \n",
    "    n_estimators = [1, 2, 4, 8, 16, 32, 64, 100, 200]\n",
    "    max_depths = numpy.linspace(1, 32, 32, endpoint=True)\n",
    "    \n",
    "    best_performance=1000\n",
    "    b_estimator=n_estimators[0]\n",
    "    b_depth=max_depths[0]\n",
    "    \n",
    "    counter=0\n",
    "    historical_performance_list=[]\n",
    "    for estimator in n_estimators:\n",
    "        for depth in max_depths:\n",
    "            for i in range(0,3):\n",
    "\n",
    "                grid_search_model=build_Random_fores_model_grid_search(int(estimator),depth)\n",
    "                grid_search_model.fit(x_train, y_train)\n",
    "                predictions = grid_search_model.predict(x_test)\n",
    "                #print(predictions)\n",
    "                # Calculate the absolute errors\n",
    "                errors = abs(predictions - y_test)\n",
    "                #print(errors.shape)\n",
    "                #print('Mean Absolute Error:', round(numpy.mean(errors), 2), 'bikes.')\n",
    "                mse = mean_squared_error(predictions, y_test)\n",
    "\n",
    "\n",
    "                counter+=1\n",
    "                if best_performance > mse:\n",
    "                    best_performance = mse\n",
    "                    b_estimator=estimator\n",
    "                    b_depth=depth\n",
    "                    print(\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\")\n",
    "                    print(mse)\n",
    "                    print(\"We have a new BEST PERFORMANCE: \"+str(best_performance))\n",
    "                    print(\"b_estimator: \"+str(b_estimator))\n",
    "                    print(\"b_depth: \"+str(b_depth))                \n",
    "                    print(\"COUNTER= \"+str(counter))\n",
    "                    historical_performance_list.append([counter,best_performance])\n",
    "                    print(\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\")\n",
    "    print(\"*****************************************************\")\n",
    "    print(\"FINALLY OUR BEST PERFORMANCE: \"+str(best_performance))\n",
    "    print(b_estimator)\n",
    "    print(b_depth)\n",
    "    print(\"We had  \"+str(counter)+\" iterations.\")\n",
    "    print(\"***************************************************************************\")\n",
    "    print(\"historical_performance_list:\")\n",
    "    print(historical_performance_list)\n",
    "    print(\"***************************************************************************\")      \n",
    "def build_XGBOOST_reg_grid_search_model(learning_r,max_depth,alpha,estimator):\n",
    "    return xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = learning_r,max_depth = max_depth, alpha = alpha, n_estimators = estimator)    \n",
    "def gridSearch_xgBoost(x_in, y_in,baseline_model):\n",
    "    test_size = 0.2\n",
    "    total = x_in.shape[0]\n",
    "    train_idx = round((1-test_size) * total)\n",
    "\n",
    "\n",
    "\n",
    "    x_train = x_in[0:train_idx]\n",
    "    #x_train = x_train.reshape(x_train.shape[0], 1, x_train.shape[1])\n",
    "\n",
    "    x_test = x_in[train_idx+1:total-1]\n",
    "    #x_test = x_test.reshape(x_test.shape[0], 1, x_test.shape[1])\n",
    "\n",
    "    y_train = y_in[0:train_idx]\n",
    "    y_test = y_in[train_idx+1:total-1]\n",
    "\n",
    "    \n",
    "    learning_rate_list = [0.01,0.03,0.1,0.3]\n",
    "    max_depths_list = [2,5,7,8]\n",
    "    alpha_list=[0,7,9,10,14]\n",
    "    n_estimators_list=[100,200,300,400]\n",
    "    \n",
    "    \n",
    "    best_performance=1000\n",
    "    b_learning_r=learning_rate_list[0]\n",
    "    b_max_depth=max_depths_list[0]\n",
    "    b_alpha=alpha_list[0]\n",
    "    b_estimator=n_estimators_list[0]\n",
    "    \n",
    "    \n",
    "    counter=0\n",
    "    historical_performance_list=[]\n",
    "    for learning_r in learning_rate_list:\n",
    "        for max_depth in max_depths_list:\n",
    "            for alpha in alpha_list:\n",
    "                for estimator in n_estimators_list:\n",
    "                    for i in range(0,2):\n",
    "                        \n",
    "                        grid_search_model=build_XGBOOST_reg_grid_search_model(learning_r,max_depth,alpha,estimator)\n",
    "                        grid_search_model.fit(x_train, y_train)\n",
    "                        predictions = grid_search_model.predict(x_test)\n",
    "                        #print(predictions)\n",
    "                        # Calculate the absolute errors\n",
    "                        errors = abs(predictions - y_test)\n",
    "                        #print(errors.shape)\n",
    "                        #print('Mean Absolute Error:', round(numpy.mean(errors), 2), 'bikes.')\n",
    "                        mse = mean_squared_error(predictions, y_test)\n",
    "\n",
    "\n",
    "                        counter+=1\n",
    "                        if best_performance > mse:\n",
    "                            best_performance = mse\n",
    "                            b_learning_r=learning_r\n",
    "                            b_max_depth=max_depth\n",
    "                            b_alpha=alpha                       \n",
    "                            b_estimator=estimator\n",
    "                            \n",
    "                            print(\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\")\n",
    "                            print(mse)\n",
    "                            print(\"We have a new BEST PERFORMANCE: \"+str(best_performance))\n",
    "                            print(\"b_learning_r: \"+str(b_learning_r))\n",
    "                            print(\"b_max_depth: \"+str(b_max_depth))                \n",
    "                            print(\"b_alpha: \"+str(b_alpha))\n",
    "                            print(\"b_estimator: \"+str(b_estimator))\n",
    "                            print(\"COUNTER= \"+str(counter))\n",
    "                            historical_performance_list.append([counter,best_performance])\n",
    "                            print(\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\")\n",
    "    print(\"*****************************************************\")\n",
    "    print(\"FINALLY OUR BEST PERFORMANCE: \"+str(best_performance))    \n",
    "    print(\"We had  \"+str(counter)+\" iterations.\")\n",
    "    print(\"***************************************************************************\")\n",
    "    print(\"historical_performance_list:\")\n",
    "    print(historical_performance_list)\n",
    "    print(\"***************************************************************************\")      \n",
    "                            \n",
    "            \n",
    "    \n",
    "def save_model(dataFrame,model_name,min_value,max_value):\n",
    "    import pickle\n",
    "    file_name_string='model_data_prediction/'+model_name+\"_predictions\"\n",
    "    if min_value!=None or max_value!=None:\n",
    "        file_name_string+=\"_from_\"+str(min_value)+\"_to_\"+str(max_value)\n",
    "    file_name_string+=\".bin\"\n",
    "    print(\"this is the filename string\")\n",
    "    print(\"Saving model on \"+file_name_string)\n",
    "   \n",
    "    dataFrame[min_value:max_value].to_pickle(file_name_string)\n",
    "def execute_Sequential_model(model,df):\n",
    "    df.set_index('timestamp_from', inplace=True)\n",
    "    df = df.sort_values(by=['timestamp_from'])\n",
    "\n",
    "    #set bike_count as Y\n",
    "    df_y = df.bike_count\n",
    "    df_x = df.drop(columns=\"bike_count\")\n",
    "    #Normalization\n",
    "    x = df_x.values\n",
    "    y = df_y.values.reshape(-1, 1)\n",
    "\n",
    "    x_scaler = preprocessing.MinMaxScaler()\n",
    "    x_normalized = x_scaler.fit_transform(x)\n",
    "\n",
    "    y_scaler = preprocessing.MinMaxScaler()\n",
    "    y_normalized = y_scaler.fit_transform(y)\n",
    "\n",
    "    x_in = pd.DataFrame(x_normalized)\n",
    "    y_in = pd.DataFrame(y_normalized)\n",
    "\n",
    "\n",
    "    #Prepare train & test dataset\n",
    "    test_size = 0.2\n",
    "    total = x_in.shape[0]\n",
    "    train_idx = round((1-test_size) * total)\n",
    "\n",
    "\n",
    "\n",
    "    x_train = x_in[0:train_idx].values\n",
    "    #x_train = x_train.reshape(x_train.shape[0], 1, x_train.shape[1])\n",
    "\n",
    "    x_test = x_in[train_idx+1:total-1].values\n",
    "    #x_test = x_test.reshape(x_test.shape[0], 1, x_test.shape[1])\n",
    "\n",
    "    y_train = y_in[0:train_idx].values\n",
    "    y_test = y_in[train_idx+1:total-1]\n",
    "\n",
    "\n",
    "\n",
    "    print(\"x_train shape {}\".format(x_train.shape))\n",
    "    print(\"y_train shape {}\".format(y_train.shape))\n",
    "    def show_loss_graph(hist,variable):\n",
    "        plt.plot(history.history[variable])    \n",
    "        plt.title('model '+variable)\n",
    "        plt.ylabel(variable)\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train '+variable], loc='upper left')\n",
    "        plt.show()\n",
    "        \n",
    "    #gridSearch_neural_network(x_train, y_train,model)\n",
    "    \n",
    "    \n",
    "    history=model.fit(x_train, y_train,epochs=100)\n",
    "    \n",
    "    model.save('.././model/model-23-jun.bin')\n",
    "    show_loss_graph(history,\"loss\")\n",
    "    show_loss_graph(history,\"mean_squared_error\")\n",
    "    def evaluate_model_and_show_graph(x_test,y_test,model,min_value=None,max_value=None):\n",
    "        model.evaluate(x_test, y_test)\n",
    "        y_prediction = model.predict(x_test)\n",
    "        print(y_prediction)\n",
    "        y_pred_scaled = y_scaler.inverse_transform(y_prediction)\n",
    "        y_test_scaled = y_scaler.inverse_transform(y_test)\n",
    "        mse = mean_squared_error(y_pred_scaled[min_value:max_value], y_test_scaled[min_value:max_value])\n",
    "        mae=mean_absolute_error(y_pred_scaled[min_value:max_value], y_test_scaled[min_value:max_value])\n",
    "        print(\"MSE: \"+str(mse))\n",
    "        print(\"MAE: \"+str(mae))\n",
    "        plt.rcParams['figure.figsize'] = [18, 18]\n",
    "        #---SAVING PREDICTIONS DATAFRAME\n",
    "        pred_df=pd.DataFrame(y_pred_scaled)     \n",
    "        y_test_df=pd.DataFrame(y_test_scaled)          \n",
    "        save_model(pred_df,'FNN',min_value,max_value)\n",
    "        \n",
    "        '''y_test_df=y_test.reset_index(inplace=False)  \n",
    "        y_test_df=y_test_df.drop(columns=\"timestamp_from\")\n",
    "        save_model(y_test_df,'Ground_t',min_value,max_value)'''\n",
    "        #---END SAVING PREDICTIONS DATAFRAME   \n",
    "        l1, = plt.plot(y_test_scaled[min_value:max_value], 'g')\n",
    "        l2, = plt.plot(pred_df[min_value:max_value] ,'r', alpha=0.7)\n",
    "        plt.legend(['Ground truth', 'Predicted'])\n",
    "        plt.show()\n",
    "    evaluate_model_and_show_graph(x_test,y_test,model)\n",
    "    update_one_hot_data('.././data/combined_data_22-jun.tsv','.././data/combined_data_22-jun_one_hot_data.tsv')\n",
    "    df_new_data = pd.read_csv('.././data/combined_data_22-jun_one_hot_data.tsv',\n",
    "                     sep='\\t',\n",
    "                     header=0)\n",
    "    start_size=df_new_data.shape[0]\n",
    "    print(df_new_data.shape)\n",
    "    df_new_data = remove_outlier(df_new_data, \"bike_count\")\n",
    "    print(\"We have removed \"+str(start_size-df_new_data.shape[0])+\" outliers tuples (which is \"+str((start_size-df_new_data.shape[0])/start_size*100)+\" % of total).\")\n",
    "    df_new_data=remove_features(df_new_data)    \n",
    "    df_new_data.set_index('timestamp_from', inplace=True)\n",
    "    df_new_data = df_new_data.sort_values(by=['timestamp_from'])\n",
    "    print(df_new_data.shape)\n",
    "\n",
    "\n",
    "    #set bike_count as Y\n",
    "    df_y = df_new_data.bike_count\n",
    "    df_x = df_new_data.drop(columns=\"bike_count\")\n",
    "\n",
    "\n",
    "    #Normalization\n",
    "    x = df_x.values\n",
    "    y = df_y.values.reshape(-1, 1)\n",
    "\n",
    "    x_scaler = preprocessing.MinMaxScaler()\n",
    "    x_normalized = x_scaler.fit_transform(x)\n",
    "\n",
    "    y_scaler = preprocessing.MinMaxScaler()\n",
    "    y_normalized = y_scaler.fit_transform(y)\n",
    "\n",
    "    x_in = pd.DataFrame(x_normalized)\n",
    "    y_in = pd.DataFrame(y_normalized)\n",
    "    #this line is commented in order to obtain the correct .bin data on data/data_model_prediction\n",
    "    #evaluate_model_and_show_graph(x_in,y_in,model)\n",
    "    print(\"You are running a Sequential model\")\n",
    "    return model\n",
    "def execute_Random_forest_model(model,df_x,df_y): \n",
    "    #gridSearch_Random_forest(df_x, df_y,model)\n",
    "    model.fit(df_x, df_y)\n",
    "    print(\"You are running a RandomForest model\")\n",
    "    return model\n",
    "def execute_SVM_regression_model(model,df_x,df_y):\n",
    "    model.fit(df_x, df_y)\n",
    "    print(\"You are running a SVM-Regression model\")\n",
    "    return model\n",
    "def execute_XGBOOST_model(model,df_x,df_y):\n",
    "    #gridSearch_xgBoost(df_x,df_y,model)\n",
    "    model.fit(df_x, df_y)\n",
    "    print(\"You are running a XGBOOST model\")\n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_test=None\n",
    "y_test=None\n",
    "print(df.shape)\n",
    "print(type(model))\n",
    "#input()\n",
    "def EXECUTE_MODEL(model,df):\n",
    "    if isinstance(model,sklearn.ensemble.forest.RandomForestRegressor):\n",
    "        df.set_index('timestamp_from', inplace=True)\n",
    "        df = df.sort_values(by=['timestamp_from'])\n",
    "        #set bike_count as Y\n",
    "        df_y = df.bike_count\n",
    "        df_x = df.drop(columns=\"bike_count\")\n",
    "        #Prepare train & test dataset\n",
    "        test_size = 0.2\n",
    "        total = df_x.shape[0]\n",
    "        train_idx = round((1-test_size) * total)\n",
    "\n",
    "\n",
    "\n",
    "        x_train = df_x[0:train_idx].values\n",
    "        #x_train = x_train.reshape(x_train.shape[0], 1, x_train.shape[1])\n",
    "        global x_test\n",
    "        global y_test\n",
    "\n",
    "        x_test = df_x[train_idx+1:total-1].values\n",
    "        #x_test = x_test.reshape(x_test.shape[0], 1, x_test.shape[1])\n",
    "\n",
    "        y_train = df_y[0:train_idx].values\n",
    "        y_test = df_y[train_idx+1:total-1]\n",
    "        return execute_Random_forest_model(model,x_train,y_train)\n",
    "    if isinstance(model,sklearn.svm.classes.SVR):\n",
    "        df.set_index('timestamp_from', inplace=True)\n",
    "        df = df.sort_values(by=['timestamp_from'])\n",
    "        #set bike_count as Y\n",
    "        df_y = df.bike_count\n",
    "        df_x = df.drop(columns=\"bike_count\")\n",
    "        #Prepare train & test dataset\n",
    "        test_size = 0.2\n",
    "        total = df_x.shape[0]\n",
    "        train_idx = round((1-test_size) * total)\n",
    "\n",
    "\n",
    "\n",
    "        x_train = df_x[0:train_idx].values\n",
    "        #x_train = x_train.reshape(x_train.shape[0], 1, x_train.shape[1])\n",
    "             \n",
    "\n",
    "        x_test = df_x[train_idx+1:total-1].values\n",
    "        #x_test = x_test.reshape(x_test.shape[0], 1, x_test.shape[1])\n",
    "\n",
    "        y_train = df_y[0:train_idx].values\n",
    "        y_test = df_y[train_idx+1:total-1]        \n",
    "        return execute_SVM_regression_model(model,x_train,y_train)\n",
    "    if isinstance(model,xgboost.sklearn.XGBRegressor):\n",
    "        df.set_index('timestamp_from', inplace=True)\n",
    "        df = df.sort_values(by=['timestamp_from'])\n",
    "        #set bike_count as Y\n",
    "        df_y = df.bike_count\n",
    "        df_x = df.drop(columns=\"bike_count\")\n",
    "        #Prepare train & test dataset\n",
    "        test_size = 0.2\n",
    "        total = df_x.shape[0]\n",
    "        train_idx = round((1-test_size) * total)\n",
    "\n",
    "\n",
    "\n",
    "        x_train = df_x[0:train_idx].values\n",
    "        #x_train = x_train.reshape(x_train.shape[0], 1, x_train.shape[1])\n",
    "        \n",
    "\n",
    "        x_test = df_x[train_idx+1:total-1].values\n",
    "        #x_test = x_test.reshape(x_test.shape[0], 1, x_test.shape[1])\n",
    "\n",
    "        y_train = df_y[0:train_idx].values\n",
    "        y_test = df_y[train_idx+1:total-1]\n",
    "        return execute_XGBOOST_model(model,x_train,y_train)        \n",
    "    else:\n",
    "        return execute_Sequential_model(model,df)\n",
    "    \n",
    "    \n",
    "#print(type(model))\n",
    "#print(df)\n",
    "model=EXECUTE_MODEL(model,df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(model,sklearn.ensemble.forest.RandomForestRegressor):\n",
    "    #print(model.feature_importances_)\n",
    "    feature_list = list(df.drop(columns=\"bike_count\").columns)\n",
    "    importances = list(model.feature_importances_)\n",
    "    # List of tuples with variable and importance\n",
    "    feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "    # Sort the feature importances by most important first\n",
    "    feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "    # Print out the feature and importances \n",
    "    [print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];\n",
    "    #print((model.feature_importances_.shape))\n",
    "    def evaluate_random_forest_model(x_test,y_test,min_value=None,max_value=None):\n",
    "        \n",
    "        predictions = model.predict(x_test)\n",
    "        #print(predictions)\n",
    "        # Calculate the absolute errors\n",
    "        errors = abs(predictions - y_test)\n",
    "        #print(errors.shape)\n",
    "        #print('Mean Absolute Error:', round(numpy.mean(errors), 2), 'bikes.')\n",
    "        mse = mean_squared_error(predictions[min_value:max_value], y_test[min_value:max_value])\n",
    "        mae=mean_absolute_error(predictions[min_value:max_value], y_test[min_value:max_value])\n",
    "        print(\"MSE\",mse)\n",
    "        print(\"MAE\",mae)\n",
    "        plt.rcParams['figure.figsize'] = [18, 18]        \n",
    "        temp_df=y_test.reset_index(inplace=False)\n",
    "        temp_df=temp_df.drop(columns=\"timestamp_from\")  \n",
    "        #---SAVING PREDICTIONS DATAFRAME\n",
    "        pred_df=pd.DataFrame(predictions)  \n",
    "        '''print(type(pred_df))\n",
    "        print(pred_df.shape)\n",
    "        print(pred_df)\n",
    "        input()'''\n",
    "        save_model(pred_df,'RandomForestRegressor',min_value,max_value)\n",
    "        y_test_df=y_test.reset_index(inplace=False)     \n",
    "        y_test_df=y_test_df.drop(columns=\"timestamp_from\")\n",
    "        save_model(y_test_df,'Ground_t',min_value,max_value)\n",
    "        #---END SAVING PREDICTIONS DATAFRAME  \n",
    "        \n",
    "        l1, = plt.plot(temp_df[min_value:max_value], 'g')\n",
    "        l2, = plt.plot(pred_df[min_value:max_value], 'r', alpha=0.7)\n",
    "        plt.legend(['Ground truth', 'Predicted'])\n",
    "        plt.show()\n",
    "    evaluate_random_forest_model(x_test,y_test)\n",
    "if isinstance(model,sklearn.svm.classes.SVR):\n",
    "    #print(model.feature_importances_)\n",
    "    \n",
    "    #print((model.feature_importances_.shape))\n",
    "    def evaluate_random_SVM_model(x_test,y_test,min_value=None,max_value=None):\n",
    "        \n",
    "        predictions = model.predict(x_test)\n",
    "        #print(predictions)\n",
    "        # Calculate the absolute errors\n",
    "        errors = abs(predictions - y_test)\n",
    "        #print(errors.shape)\n",
    "        #print('Mean Absolute Error:', round(numpy.mean(errors), 2), 'bikes.')\n",
    "        mse = mean_squared_error(predictions[min_value:max_value], y_test[min_value:max_value])\n",
    "        mae=mean_absolute_error(predictions[min_value:max_value], y_test[min_value:max_value])\n",
    "        print(\"MSE\",mse)\n",
    "        print(\"MAE\",mae)\n",
    "        plt.rcParams['figure.figsize'] = [18, 18]        \n",
    "        temp_df=y_test.reset_index(inplace=False)\n",
    "        temp_df=temp_df.drop(columns=\"timestamp_from\")  \n",
    "        #---SAVING PREDICTIONS DATAFRAME\n",
    "        pred_df=pd.DataFrame(predictions)      \n",
    "        save_model(pred_df,'SVR',min_value,max_value)\n",
    "        y_test_df=y_test.reset_index(inplace=False)  \n",
    "        y_test_df=y_test_df.drop(columns=\"timestamp_from\")\n",
    "        save_model(y_test_df,'Ground_t',min_value,max_value)\n",
    "        #---END SAVING PREDICTIONS DATAFRAME        \n",
    "        l1, = plt.plot(temp_df[min_value:max_value], 'g')\n",
    "        l2, = plt.plot(pred_df[min_value:max_value], 'r', alpha=0.7)\n",
    "        plt.legend(['Ground truth', 'Predicted'])\n",
    "        plt.show()\n",
    "    evaluate_random_SVM_model(x_test,y_test)\n",
    "if isinstance(model,xgboost.sklearn.XGBRegressor):\n",
    "    def evaluate_xgboost_model(x_test,y_test,min_value=None,max_value=None):\n",
    "        \n",
    "        predictions = model.predict(x_test)\n",
    "        print(type(predictions))\n",
    "        \n",
    "        #print(predictions)\n",
    "        # Calculate the absolute errors\n",
    "        predictions[predictions<0]=0\n",
    "        errors = abs(predictions[min_value:max_value] - y_test[min_value:max_value])\n",
    "        #print(errors.shape)\n",
    "        #print('Mean Absolute Error:', round(numpy.mean(errors), 2), 'bikes.')\n",
    "        mse = mean_squared_error(predictions[min_value:max_value], y_test[min_value:max_value])\n",
    "        mae=mean_absolute_error(predictions[min_value:max_value], y_test[min_value:max_value])\n",
    "        print(\"MSE\",mse)\n",
    "        print(\"MAE\",mae)\n",
    "        plt.rcParams['figure.figsize'] = [18, 18]        \n",
    "        temp_df=y_test.reset_index(inplace=False)\n",
    "        temp_df=temp_df.drop(columns=\"timestamp_from\")         \n",
    "        #---SAVING PREDICTIONS DATAFRAME\n",
    "        \n",
    "        pred_df=pd.DataFrame(predictions) \n",
    "        \n",
    "        \n",
    "        save_model(pred_df,'XGBRegressor',min_value,max_value)\n",
    "        y_test_df=y_test.reset_index(inplace=False)   \n",
    "        y_test_df=y_test_df.drop(columns=\"timestamp_from\")\n",
    "        save_model(y_test_df,'Ground_t',min_value,max_value)\n",
    "        #---END SAVING PREDICTIONS DATAFRAME\n",
    "        l1, = plt.plot(temp_df[min_value:max_value], 'g')\n",
    "        l2, = plt.plot(pred_df[min_value:max_value], 'r', alpha=0.7)\n",
    "        plt.legend(['Ground truth', 'Predicted'])\n",
    "        plt.show()    \n",
    "    evaluate_xgboost_model(x_test,y_test)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " print(dt.now()-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
